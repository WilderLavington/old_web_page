{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling optimal trajectories \n",
    "Now that we have established the corrosponding classicical methodologies (max entropy RL with GAE), we can build to using variational inference to determine optimal policies. In order to do this, we assume that we can sample optimal policies by interacting with the enviroment. In practice, this just means that we create an agent that performs uniformly random actions within the space, generate trajctories from this agent, and then sample those trajectories based upon the pdf. This pdf is given in https://arxiv.org/abs/1805.00909, and efffectively creates a product of exponential rewards associated with each state. In this way, trajectories that yeilded higher rewards are more likly to be sampled. In order to sample these trajectories, I apply the Metropolis-Hastings Algorithm, however in the future I might impliment Metropolis-in-Gibbs or Hamiltonian-Monte-Carlo. The first chunk of code below is the random walk policy discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run running_codebase.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANDOM_WALK(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(RANDOM_WALK, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "    def sample_action(self, state):\n",
    "        probabilities = torch.ones(self.action_size) / self.action_size\n",
    "        action = torch.distributions.Categorical(probabilities)\n",
    "        return action.sample()\n",
    "        \n",
    "    def logprob_action(self, state, action):\n",
    "        probabilities = torch.ones(state_size)/state_size\n",
    "        action = torch.distributions.Categorical(probabilities)\n",
    "        log_prob = min(0, torch.log(probabilities[action.long()]+0.0001))\n",
    "        return log_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling full trajectories\n",
    "Now that we have a random walk policy we can start sampling trajectoriess. We will start by attempting to sample full trajectories of the expert based upon the reward function prior to moving onto state-action pairs. In order to do this, all we need is we will apply the random walk policy and sample trajectories that yeilded good rewards. An interesting point would be to sample states with a high value function instead reward. and then sample based upon that. This might be somewhart easier for state action sampling as it adds a chain of causality and wont allow the true distribution density so senter around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_optimal_trajectory(samples, T, grid, reward_shape, iterations, burn_in):\n",
    "    \n",
    "    # initialize random policy \n",
    "    rw = RANDOM_WALK(2,5)\n",
    "    policy = lambda state: rw.sample_action(state)\n",
    "    \n",
    "    # initialize joint pdf function Liklyhood*prior\n",
    "    log_pdf = lambda action_size, rewards: torch.tensor([ \\\n",
    "        torch.log(torch.tensor(torch.exp(reward)*(1/action_size))) \\\n",
    "        for reward in rewards])\n",
    "    \n",
    "    # log joint pdf \n",
    "    log_joint_pdf = lambda action_size, rewards: sum(log_pdf(action_size, rewards))\n",
    "    \n",
    "    # get initial sample info\n",
    "    state_tensor, action_tensor, reward_tensor = grid.simulate_trajectory(T, 1, policy)\n",
    "    state_samples = torch.zeros((2,T,iterations))\n",
    "    action_samples = torch.zeros((1,T,iterations))\n",
    "    initial_sample_pdf = log_joint_pdf(2, reward_tensor)\n",
    "    \n",
    "    # initial state\n",
    "    state_samples[:,:,0] = state_tensor.reshape(2,T)\n",
    "    action_samples[:,:,0] = action_tensor.reshape(1,T)\n",
    "        \n",
    "    # sample trajectory from the posterior\n",
    "    for i in range(1, iterations):\n",
    "        \n",
    "        # get sampled trajectories\n",
    "        state_tensor, action_tensor, reward_tensor = grid.simulate_trajectory(T, 1, policy) \n",
    "        \n",
    "        # reshape rewards \n",
    "        reward_tensor = torch.tensor([[reward_shape(reward_t) for reward_t in reward_ts] \\\n",
    "                                      for reward_ts in reward_tensor])\n",
    "        \n",
    "        # get pdf of new sample\n",
    "        new_sample_pdf = log_joint_pdf(2, reward_tensor)\n",
    "        \n",
    "        # get acceptance ratio\n",
    "        A_ratio = torch.tensor(min(0.0, new_sample_pdf - initial_sample_pdf))\n",
    "        \n",
    "        # create uniform rv\n",
    "        unif = torch.rand(1)\n",
    "        \n",
    "        # accept / reject\n",
    "        if unif <= torch.exp(A_ratio):\n",
    "            \n",
    "            # update current pdf\n",
    "            initial_sample_pdf = new_sample_pdf\n",
    "            \n",
    "            # append new tensors to the current set of samples\n",
    "            state_samples[:,:,i] = state_tensor.reshape(2,T)\n",
    "            action_samples[:,:,i] = action_tensor.reshape(1,T)\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            # append old tensors to the current set of samples\n",
    "            state_samples[:,:,i] = state_samples[:,:,i-1].reshape(2,T)\n",
    "            action_samples[:,:,i] = action_samples[:,:,i-1].reshape(1,T)\n",
    "            \n",
    "    # now we burn in\n",
    "    \n",
    "    return state_samples[:,:,burn_in:], action_samples[:,:,burn_in:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 0.])\n",
      "tensor([4., 0.])\n",
      "tensor([3., 1.])\n",
      "tensor([2., 1.])\n"
     ]
    }
   ],
   "source": [
    "pre_determined_grid = [[-1,   -1,  -1,   -10,   -1, 10**4],\n",
    "                       [-100, -1,  -10,  -1,   -10, -10],\n",
    "                       [-1,   -10, -100, -10,  -1, -1],\n",
    "                       [-1,   -1,  -100, -10,  -1, -1],\n",
    "                       [-10,  -1,  -100, -100, -100, -1],\n",
    "                       [-1,   -1,  -100, -100, -100, -1]]\n",
    "\n",
    "# hyper parameters\n",
    "T = 100\n",
    "samples = 1\n",
    "reward_shape = lambda reward: reward*100\n",
    "\n",
    "# markov chain parameters\n",
    "iterations = 3000\n",
    "burn_in = 1999\n",
    "\n",
    "#initializations\n",
    "start_state = [5,0]\n",
    "policy_obj = POLICY(2, 5)\n",
    "grid = GRID_WORLD(pre_determined_grid, start_state)\n",
    "\n",
    "# sample optimal trajectories\n",
    "state_samples, action_samples = sample_optimal_trajectory(samples, T, grid, reward_shape, iterations, burn_in)\n",
    "print(state_samples[:,-1,-1])\n",
    "print(state_samples[:,-3,-1])\n",
    "print(state_samples[:,-9,-1])\n",
    "print(state_samples[:,-13,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Action based sampling \n",
    "Now that we have successfully implimented the full trajectory code, we can work with the local sampling code from before. As can be seen above, when we deal with full trajectories, the agent tends to only consider trajectories that are \"safe\", as the cumulative reward over long periods of time can vary widely. In order to gaurentee that we will see optimal states that wont be eclipsed by low intermediary state rewards, we will now only look at individual state action pairs. Origininally I was somewhat worried that the setup would over sample values that were high reward without incorporating intermediary states, however if we apply metropolis hastings for each timestep (and in doing so sample states that also have an associated time), we avoid this issue for the most part. It does however only sample values along the trajectory that have the lowest value for that time step, which gives us effectively non-continuous trajectories! Is this an issue? Not for what we will be using these trajectories for (at least for the simple grid world)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_optimal_state_actions(samples, T, grid, reward_shape, burn_in):\n",
    "    \n",
    "    # initialize random policy \n",
    "    rw = RANDOM_WALK(2,5)\n",
    "    policy = lambda state: rw.sample_action(state)\n",
    "    \n",
    "    # initialize pdf function Liklyhood*prior\n",
    "    log_pdf = lambda action_size, reward: \\\n",
    "        torch.log(torch.min(torch.tensor([torch.exp(reward)*(1/action_size), 1.])))\n",
    "    \n",
    "    # initialize sample info\n",
    "    state_samples = torch.zeros((2,T,samples))\n",
    "    action_samples = torch.zeros((T,samples))\n",
    "    reward_samples = torch.zeros((T,samples)) \n",
    "    \n",
    "    # get sampled trajectories\n",
    "    state_tensor, action_tensor, reward_tensor = grid.simulate_trajectory(T, samples, policy) \n",
    "    \n",
    "    # shape reward tensor\n",
    "    reward_tensor = torch.tensor([[reward_shape(reward_i) for reward_i in rewards] for rewards in reward_tensor])\n",
    "    \n",
    "    # iterate through all timesteps\n",
    "    for t in range(T):\n",
    "        \n",
    "        # initial state at time t\n",
    "        state_samples[:,t,0] = state_tensor[:,t,0]\n",
    "        action_samples[t,0] = action_tensor[t,0]\n",
    "        reward_samples[t,0] = reward_tensor[t,0]\n",
    "        \n",
    "        # initial log pdf at time t\n",
    "        initial_sample_pdf = log_pdf(2, reward_tensor[t,0])\n",
    "        \n",
    "        # sample trajectory from the posterior at time t\n",
    "        for i in range(1, samples):\n",
    "            \n",
    "            # get pdf of new sample\n",
    "            new_sample_pdf = log_pdf(2, reward_tensor[t,i])\n",
    "\n",
    "            # get acceptance ratio\n",
    "            A_ratio =   torch.tensor(min(0.0, new_sample_pdf - initial_sample_pdf))\n",
    "            \n",
    "            # create uniform rv\n",
    "            unif = torch.rand(1)\n",
    "\n",
    "            # accept / reject\n",
    "            if unif <= torch.exp(A_ratio):\n",
    "                \n",
    "                # update current pdf\n",
    "                initial_sample_pdf = new_sample_pdf\n",
    "    \n",
    "                # append new tensors to the current set of samples\n",
    "                state_samples[:,t,i] = state_tensor[:,t,i]\n",
    "                action_samples[t,i] = action_tensor[t,i]\n",
    "                reward_samples[t,i] = reward_tensor[t,i]\n",
    "                \n",
    "            else: \n",
    "\n",
    "                # append old tensors to the current set of samples\n",
    "                state_samples[:,t,i] = state_samples[:,t,i-1]\n",
    "                action_samples[t,i] = action_samples[t,i-1]\n",
    "                reward_samples[t,0] = reward_tensor[t,i-1]\n",
    "        \n",
    "    # now we burn in\n",
    "    return state_samples[:,:,burn_in:], action_samples[:,burn_in:], reward_samples[:,burn_in:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 5.])\n",
      "tensor([0., 5.])\n",
      "tensor([0., 5.])\n",
      "tensor([0., 5.])\n"
     ]
    }
   ],
   "source": [
    "pre_determined_grid = [[-1,   -1,  -1,   -10,   -1, 10**4],\n",
    "                       [-100, -1,  -10,  -1,   -10, -10],\n",
    "                       [-1,   -10, -100, -10,  -1, -1],\n",
    "                       [-1,   -1,  -100, -10,  -1, -1],\n",
    "                       [-10,  -1,  -100, -100, -100, -1],\n",
    "                       [-1,   -1,  -100, -100, -100, -1]]\n",
    "# hyper parameters\n",
    "T = 100\n",
    "samples = 2000\n",
    "reward_shape = lambda reward: reward*10 \n",
    "\n",
    "# markov chain parameters\n",
    "burn_in = 999\n",
    "\n",
    "#initializations\n",
    "start_state = [5,0]\n",
    "policy_obj = POLICY(2, 5)\n",
    "grid = GRID_WORLD(pre_determined_grid, start_state)\n",
    "\n",
    "# sample optimal trajectories\n",
    "state_samples, action_samples, reward_samples = sample_optimal_state_actions(samples, T, grid, reward_shape, burn_in)\n",
    "\n",
    "# check a couple of the last states to be sampled\n",
    "print(state_samples[:,-1,-1])\n",
    "print(state_samples[:,-3,-1])\n",
    "print(state_samples[:,-9,-1])\n",
    "print(state_samples[:,-13,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats next?\n",
    "Now that we have a slick way to sample optimal trajectories within our space, what are we going to do with this information? Well ideally we want to be able to create a policy that can solve this maze, and now that we can sample these (albeit discontinuous) trajectories why not just try to match those! Next time we will set up the variational inference problem, and see if we can train a variational distribution to match the sampled one that we just create!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
